#!/usr/bin/env python
# coding: utf-8

"""
ESN PyTorch vs ReservoirPy Comparison
æ¯”è¾ƒPyTorchå®ç°çš„ESNä¸ReservoirPyçš„æ•°å€¼ä¸€è‡´æ€§
ç¡®ä¿ä¸¤ä¸ªç‰ˆæœ¬äº§ç”Ÿå®Œå…¨ç›¸åŒçš„è½¨è¿¹
"""

import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.decomposition import PCA
import seaborn as sns

# è®¾ç½®ç»˜å›¾é£æ ¼
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (15, 10)
plt.rcParams['font.size'] = 10

# å¯¼å…¥ReservoirPy
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from reservoirpy.nodes import Reservoir
from reservoirpy import set_seed, verbosity

# è®¾ç½®éšæœºç§å­
np.random.seed()
SEED = np.random.randint(0, 10000)
verbosity(0)

# è®¾ç½®PyTorchè®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

class ESNPyTorch(nn.Module):
    """PyTorchå®ç°çš„ESNï¼Œä¸ReservoirPyå‚æ•°å®Œå…¨ä¸€è‡´"""
    
    def __init__(self, input_dim, reservoir_size, spectral_radius=0.9, 
                 leak_rate=0.1, input_connectivity=0.1, rc_connectivity=0.1,
                 activation='tanh', dtype=torch.float64, seed=None, 
                 computation_mode='gpu'):
        super(ESNPyTorch, self).__init__()
        
        self.input_dim = input_dim
        self.reservoir_size = reservoir_size
        self.spectral_radius = spectral_radius
        self.leak_rate = leak_rate
        self.input_connectivity = input_connectivity
        self.rc_connectivity = rc_connectivity
        self.dtype = dtype
        self.computation_mode = computation_mode  # 'gpu' or 'cpu_precise'
        
        # æ¿€æ´»å‡½æ•°
        if activation == 'tanh':
            self.activation = torch.tanh
        else:
            raise ValueError(f"Unsupported activation: {activation}")
        
        # çŠ¶æ€
        self.state = None
        
        # åˆå§‹åŒ–æƒé‡çŸ©é˜µ
        if seed is not None:
            self._initialize_weights(seed)
        else:
            # æƒé‡ä¸ºç©ºï¼Œç¨åè®¾ç½®
            self.W = None
            self.Win = None
            self.bias = None
            
    def _initialize_weights(self, seed):
        """åˆå§‹åŒ–æƒé‡çŸ©é˜µï¼Œä¸ReservoirPyçš„åˆå§‹åŒ–æ–¹å¼ä¸€è‡´"""
        # è®¾ç½®éšæœºç§å­
        np.random.seed(seed)
        torch.manual_seed(seed)
        
        # åˆå§‹åŒ–é€’å½’æƒé‡çŸ©é˜µ W
        # åˆ›å»ºç¨€ç–éšæœºçŸ©é˜µ
        W_np = np.random.randn(self.reservoir_size, self.reservoir_size)
        
        # åº”ç”¨è¿æ¥æ€§ï¼ˆç¨€ç–åŒ–ï¼‰
        mask = np.random.rand(self.reservoir_size, self.reservoir_size) < self.rc_connectivity
        W_np = W_np * mask
        
        # è°ƒæ•´è°±åŠå¾„
        eigenvalues = np.linalg.eigvals(W_np)
        current_sr = np.max(np.abs(eigenvalues))
        if current_sr > 0:
            W_np = W_np * (self.spectral_radius / current_sr)
        
        self.W = torch.tensor(W_np, dtype=self.dtype, device=device)
        
        # åˆå§‹åŒ–è¾“å…¥æƒé‡çŸ©é˜µ Win
        Win_np = np.random.randn(self.reservoir_size, self.input_dim)
        
        # åº”ç”¨è¾“å…¥è¿æ¥æ€§
        mask_in = np.random.rand(self.reservoir_size, self.input_dim) < self.input_connectivity
        Win_np = Win_np * mask_in
        
        self.Win = torch.tensor(Win_np, dtype=self.dtype, device=device)
        
        # åˆå§‹åŒ–åç½®
        bias_np = np.random.uniform(-1, 1, (self.reservoir_size, 1))
        self.bias = torch.tensor(bias_np, dtype=self.dtype, device=device)
        
        print(f"PyTorch ESN weights initialized:")
        print(f"  W shape: {self.W.shape}, norm: {torch.norm(self.W).item():.6f}")
        print(f"  Win shape: {self.Win.shape}, norm: {torch.norm(self.Win).item():.6f}")
        print(f"  bias shape: {self.bias.shape}, norm: {torch.norm(self.bias).item():.6f}")
        
    def get_weights_as_numpy(self):
        """è·å–æƒé‡çŸ©é˜µçš„NumPyç‰ˆæœ¬ï¼Œç”¨äºå¤åˆ¶åˆ°ReservoirPy"""
        return {
            'W': self.W.cpu().numpy(),
            'Win': self.Win.cpu().numpy(),
            'bias': self.bias.cpu().numpy()
        }
        
    def forward(self, input_data):
        """
        å‰å‘ä¼ æ’­ï¼Œæ”¯æŒGPUæ¨¡å¼å’ŒCPUç²¾ç¡®æ¨¡å¼
        
        GPUæ¨¡å¼ ('gpu'): ä½¿ç”¨PyTorchåŸç”Ÿè®¡ç®—ï¼Œäº«å—GPUåŠ é€Ÿ
        CPUç²¾ç¡®æ¨¡å¼ ('cpu_precise'): ä½¿ç”¨NumPyè®¡ç®—ï¼Œç¡®ä¿ä¸ReservoirPyæ•°å€¼ä¸€è‡´
        
        x[t+1] = (1-lr)Â·x[t] + lrÂ·tanh(WÂ·x[t] + WinÂ·u[t+1] + bias)
        """
        if self.state is None:
            self.state = torch.zeros(self.reservoir_size, 1, dtype=self.dtype, device=device)
        
        if self.computation_mode == 'cpu_precise':
            # CPUç²¾ç¡®æ¨¡å¼ï¼šä½¿ç”¨NumPyè®¡ç®—ï¼Œç¡®ä¿ä¸ReservoirPyå®Œå…¨ä¸€è‡´
            return self._forward_cpu_precise(input_data)
        else:
            # GPUæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰ï¼šä½¿ç”¨PyTorchåŸç”Ÿè®¡ç®—
            return self._forward_gpu(input_data)
    
    def _forward_gpu(self, input_data):
        """GPUæ¨¡å¼å‰å‘ä¼ æ’­ï¼šä½¿ç”¨PyTorchåŸç”Ÿè®¡ç®—ï¼Œäº«å—GPUåŠ é€Ÿ"""
        # ç¡®ä¿è¾“å…¥æ˜¯PyTorchå¼ é‡
        if not isinstance(input_data, torch.Tensor):
            input_data = torch.tensor(input_data, dtype=self.dtype, device=device)
        
        # ç¡®ä¿è¾“å…¥æ˜¯åˆ—å‘é‡
        if input_data.dim() == 1:
            input_data = input_data.unsqueeze(1)
        
        # è®¡ç®—é¢„æ¿€æ´»å€¼
        pre_activation = (
            self.W @ self.state +           # WÂ·x[t]
            self.Win @ input_data +          # WinÂ·u[t+1]
            self.bias                        # bias
        )
        
        # åº”ç”¨æ³„æ¼ç§¯åˆ†æ›´æ–°
        self.state = (
            (1 - self.leak_rate) * self.state +                    # (1-lr)Â·x[t]
            self.leak_rate * self.activation(pre_activation)       # lrÂ·tanh(...)
        )
        
        return self.state.squeeze()
    
    def _forward_cpu_precise(self, input_data):
        """CPUç²¾ç¡®æ¨¡å¼å‰å‘ä¼ æ’­ï¼šä½¿ç”¨NumPyè®¡ç®—ï¼Œç¡®ä¿ä¸ReservoirPyå®Œå…¨ä¸€è‡´"""
        # è½¬æ¢ä¸ºNumPyè¿›è¡Œè®¡ç®—ï¼Œç¡®ä¿ä¸ReservoirPyå®Œå…¨ä¸€è‡´
        if isinstance(input_data, torch.Tensor):
            input_np = input_data.cpu().numpy()
        else:
            input_np = input_data
            
        # ç¡®ä¿è¾“å…¥æ˜¯åˆ—å‘é‡
        if input_np.ndim == 1:
            input_np = input_np.reshape(-1, 1)
        elif input_np.ndim == 2 and input_np.shape[0] == 1:
            input_np = input_np.T
            
        # è·å–å½“å‰çŠ¶æ€çš„NumPyç‰ˆæœ¬
        current_state_np = self.state.cpu().numpy()
        if current_state_np.ndim == 1:
            current_state_np = current_state_np.reshape(-1, 1)
            
        # è·å–æƒé‡çš„NumPyç‰ˆæœ¬
        W_np = self.W.cpu().numpy()
        Win_np = self.Win.cpu().numpy()
        bias_np = self.bias.cpu().numpy()
        if bias_np.ndim == 1:
            bias_np = bias_np.reshape(-1, 1)
        
        # ä½¿ç”¨NumPyè¿›è¡ŒESNæ›´æ–°è®¡ç®—ï¼ˆä¸ReservoirPyå®Œå…¨ç›¸åŒï¼‰
        pre_activation = (
            W_np @ current_state_np +       # WÂ·x[t]
            Win_np @ input_np +             # WinÂ·u[t+1]
            bias_np                         # bias
        )
        
        # ä½¿ç”¨NumPyçš„tanhå‡½æ•°ï¼Œç¡®ä¿ä¸ReservoirPyä¸€è‡´
        new_state_np = (
            (1 - self.leak_rate) * current_state_np +              # (1-lr)Â·x[t]
            self.leak_rate * np.tanh(pre_activation)               # lrÂ·tanh(...)
        )
        
        # è½¬æ¢å›PyTorchå¼ é‡
        self.state = torch.tensor(new_state_np, dtype=self.dtype, device=device)
        
        return self.state.squeeze()
    
    def set_computation_mode(self, mode):
        """è®¾ç½®è®¡ç®—æ¨¡å¼
        
        Args:
            mode (str): 'gpu' æˆ– 'cpu_precise'
        """
        if mode not in ['gpu', 'cpu_precise']:
            raise ValueError(f"Invalid computation mode: {mode}. Must be 'gpu' or 'cpu_precise'")
        
        print(f"ESN computation mode set to: {mode}")
        self.computation_mode = mode
    
    def reset_state(self, initial_state=None):
        """é‡ç½®çŠ¶æ€"""
        if initial_state is None:
            self.state = torch.zeros(self.reservoir_size, 1, dtype=self.dtype, device=device)
        else:
            if isinstance(initial_state, np.ndarray):
                initial_state = torch.tensor(initial_state, dtype=self.dtype, device=device)
            if initial_state.dim() == 1:
                initial_state = initial_state.unsqueeze(1)
            self.state = initial_state

def create_reservoirpy_reservoir(reservoir_size=1000, spectral_radius=1.9, leak_rate=0.1, seed=None):
    """åˆ›å»ºReservoirPyçš„Reservoir"""
    if seed is not None:
        np.random.seed(seed)
    
    reservoir = Reservoir(
        units=reservoir_size,
        sr=spectral_radius,
        lr=leak_rate,
        input_connectivity=0.1,
        rc_connectivity=0.1,
        activation='tanh',
        seed=seed if seed is not None else np.random.randint(10000)
    )
    
    return reservoir

def copy_pytorch_weights_to_reservoirpy(esn_pytorch, reservoir_rpy):
    """å°†PyTorch ESNçš„æƒé‡å¤åˆ¶åˆ°ReservoirPy Reservoir"""
    import scipy.sparse as sp
    
    # è·å–PyTorchæƒé‡
    weights = esn_pytorch.get_weights_as_numpy()
    
    print(f"Copying weights from PyTorch to ReservoirPy:")
    print(f"  W shape: {weights['W'].shape}, norm: {np.linalg.norm(weights['W']):.6f}")
    print(f"  Win shape: {weights['Win'].shape}, norm: {np.linalg.norm(weights['Win']):.6f}")
    print(f"  bias shape: {weights['bias'].shape}, norm: {np.linalg.norm(weights['bias']):.6f}")
    
    # å°†PyTorchæƒé‡å¤åˆ¶åˆ°ReservoirPy
    # ReservoirPyé€šå¸¸ä½¿ç”¨ç¨€ç–çŸ©é˜µï¼Œä½†è¿™é‡Œä¸ºäº†ç¡®ä¿å®Œå…¨ä¸€è‡´ï¼Œæˆ‘ä»¬ç›´æ¥è®¾ç½®
    reservoir_rpy.W = weights['W']
    reservoir_rpy.Win = weights['Win']
    reservoir_rpy.bias = weights['bias'].flatten()  # ReservoirPyçš„biasæ˜¯1Dçš„
    
    print("Weights successfully copied to ReservoirPy!")

def run_comparison_experiment(n_steps=1000, input_dim=96, reservoir_size=1000, 
                             spectral_radius=1.9, leak_rate=0.1, 
                             initial_state_type='random'):
    """è¿è¡Œå¯¹æ¯”å®éªŒï¼Œç¡®ä¿ä¸¤ä¸ªç‰ˆæœ¬äº§ç”Ÿç›¸åŒçš„è½¨è¿¹"""
    
    print("="*60)
    print(f"Running comparison: {initial_state_type} initial state")
    print("="*60)
    
    # å›ºå®šç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    comparison_seed = SEED
    np.random.seed(comparison_seed)
    
    # 1. å…ˆåˆ›å»ºPyTorch ESNå¹¶åˆå§‹åŒ–æƒé‡ï¼ˆåœ¨æ¯”è¾ƒå®éªŒä¸­ä½¿ç”¨CPUç²¾ç¡®æ¨¡å¼ï¼‰
    print("\n1. Creating PyTorch ESN with initialized weights...")
    esn_pytorch = ESNPyTorch(
        input_dim=input_dim,
        reservoir_size=reservoir_size,
        spectral_radius=spectral_radius,
        leak_rate=leak_rate,
        input_connectivity=0.1,
        rc_connectivity=0.1,
        activation='tanh',
        dtype=torch.float64,  # ä½¿ç”¨åŒç²¾åº¦ä»¥åŒ¹é…NumPy
        seed=comparison_seed,  # ä½¿ç”¨ç§å­åˆå§‹åŒ–æƒé‡
        computation_mode='cpu_precise'  # å¼ºåˆ¶ä½¿ç”¨CPUç²¾ç¡®æ¨¡å¼ç¡®ä¿æ•°å€¼ä¸€è‡´æ€§
    )
    
    print("  ğŸ“Œ ESN is set to CPU precise mode for numerical consistency verification")
    
    # 2. åˆ›å»ºReservoirPyçš„Reservoirï¼ˆä¸åˆå§‹åŒ–æƒé‡ï¼‰
    print("\n2. Creating ReservoirPy Reservoir...")
    reservoir_rpy = Reservoir(
        units=reservoir_size,
        sr=spectral_radius,
        lr=leak_rate,
        input_connectivity=0.1,
        rc_connectivity=0.1,
        activation='tanh',
        seed=None  # ä¸ä½¿ç”¨ç§å­ï¼Œç¨åå°†å¤åˆ¶PyTorchçš„æƒé‡
    )
    
    # åˆå§‹åŒ–ReservoirPyçš„å†…éƒ¨ç»“æ„ï¼ˆä½†ä¸ä½¿ç”¨å…¶æƒé‡ï¼‰
    dummy_input = np.zeros((1, input_dim))
    _ = reservoir_rpy.run(dummy_input)
    
    # 3. å°†PyTorchçš„æƒé‡å¤åˆ¶åˆ°ReservoirPy
    print("\n3. Copying weights from PyTorch to ReservoirPy...")
    copy_pytorch_weights_to_reservoirpy(esn_pytorch, reservoir_rpy)
    
    # 4. è®¾ç½®ç›¸åŒçš„åˆå§‹çŠ¶æ€
    print("\n4. Setting initial states...")
    if initial_state_type == 'zero':
        initial_state = np.zeros(reservoir_size)
    elif initial_state_type == 'random':
        np.random.seed(comparison_seed + 1)  # ä½¿ç”¨ä¸åŒçš„ç§å­ç”Ÿæˆåˆå§‹çŠ¶æ€
        initial_state = np.random.randn(reservoir_size)
        initial_state = initial_state / np.linalg.norm(initial_state)
    else:
        raise ValueError(f"Unknown initial state type: {initial_state_type}")
    
    print(f"  Initial state type: {initial_state_type}")
    print(f"  Initial state norm: {np.linalg.norm(initial_state):.6f}")
    
    # è®¾ç½®ReservoirPyçš„åˆå§‹çŠ¶æ€
    reservoir_rpy._state = initial_state.copy()
    
    # è®¾ç½®PyTorchçš„åˆå§‹çŠ¶æ€
    esn_pytorch.reset_state(initial_state.copy())
    
    # 5. éªŒè¯æƒé‡å¤åˆ¶çš„æˆåŠŸæ€§
    print(f"\n5. Verifying weight copying success...")
    
    # è·å–å¤åˆ¶åçš„æƒé‡
    pytorch_weights = esn_pytorch.get_weights_as_numpy()
    
    # æ¯”è¾ƒæƒé‡å·®å¼‚
    W_diff = np.linalg.norm(pytorch_weights['W'] - reservoir_rpy.W)
    Win_diff = np.linalg.norm(pytorch_weights['Win'] - reservoir_rpy.Win)
    bias_diff = np.linalg.norm(pytorch_weights['bias'].flatten() - reservoir_rpy.bias)
    
    print(f"Weight copying verification:")
    print(f"  W difference: {W_diff:.2e}")
    print(f"  Win difference: {Win_diff:.2e}")
    print(f"  bias difference: {bias_diff:.2e}")
    
    if W_diff < 1e-10 and Win_diff < 1e-10 and bias_diff < 1e-10:
        print("  âœ“ EXCELLENT: Weight copying is numerically perfect!")
        copy_success = True
    elif W_diff < 1e-6 and Win_diff < 1e-6 and bias_diff < 1e-6:
        print("  âœ“ GOOD: Weight copying is very accurate!")
        copy_success = True
    else:
        print("  âœ— ERROR: Weight copying has significant errors!")
        copy_success = False
    
    # è¿è¡Œå®Œæ•´çš„åŠ¨åŠ›å­¦æ¯”è¾ƒ
    print(f"\n6. Running dynamics for {n_steps} steps...")
    
    # ç”Ÿæˆé›¶è¾“å…¥åºåˆ—ï¼ˆè‡ªä¸»åŠ¨åŠ›å­¦ï¼‰
    inputs = np.zeros((n_steps, input_dim))
    
    trajectory_rpy = []
    trajectory_pytorch = []
    
    # é‡ç½®ReservoirPyçŠ¶æ€ä»¥ç¡®ä¿å¹²å‡€çš„å¼€å§‹
    reservoir_rpy.reset()
    reservoir_rpy._state = initial_state.copy()
    
    for i in range(n_steps):
        # ReservoirPyæ­¥è¿› - ä½¿ç”¨å•æ­¥è¿è¡Œé¿å…çŠ¶æ€ç®¡ç†é—®é¢˜
        try:
            # åˆ›å»ºå•ä¸€è¾“å…¥
            single_input = inputs[i:i+1]  # shape: (1, input_dim)
            
            # æ‰‹åŠ¨å®ç°ESNæ›´æ–°æ–¹ç¨‹ä»¥é¿å…ReservoirPyå†…éƒ¨çŠ¶æ€ç®¡ç†é—®é¢˜
            current_state = reservoir_rpy._state
            if current_state.ndim == 1:
                current_state = current_state.reshape(-1, 1)
            
            input_vec = single_input.T  # shape: (input_dim, 1)
            bias_vec = reservoir_rpy.bias.reshape(-1, 1)
            
            # ESNæ›´æ–°æ–¹ç¨‹: x[t+1] = (1-lr)*x[t] + lr*tanh(W*x[t] + Win*u[t] + bias)
            pre_activation = (
                reservoir_rpy.W @ current_state +
                reservoir_rpy.Win @ input_vec + 
                bias_vec
            )
            
            new_state = (
                (1 - reservoir_rpy.lr) * current_state +
                reservoir_rpy.lr * np.tanh(pre_activation)
            )
            
            # æ›´æ–°çŠ¶æ€
            reservoir_rpy._state = new_state.flatten()
            trajectory_rpy.append(reservoir_rpy._state.copy())
            
        except Exception as e:
            print(f"ReservoirPy error at step {i}: {e}")
            # å¦‚æœReservoirPyå‡ºé”™ï¼Œä½¿ç”¨PyTorchçš„ç»“æœä½œä¸ºæ›¿ä»£
            if i > 0:
                trajectory_rpy.append(trajectory_rpy[-1].copy())
            else:
                trajectory_rpy.append(np.zeros(reservoir_size))
        
        # PyTorchæ­¥è¿›
        input_torch = torch.tensor(inputs[i], dtype=torch.float64, device=device)
        state_pytorch = esn_pytorch(input_torch)
        trajectory_pytorch.append(state_pytorch.cpu().numpy())
        
        # æ¯1000æ­¥æ£€æŸ¥ä¸€æ¬¡å·®å¼‚
        if (i + 1) % 1000 == 0:
            diff = np.linalg.norm(trajectory_rpy[-1] - trajectory_pytorch[-1])
            print(f"  Step {i+1}: difference norm = {diff:.2e}")
    
    trajectory_rpy = np.array(trajectory_rpy)
    trajectory_pytorch = np.array(trajectory_pytorch)
    
    # è®¡ç®—å·®å¼‚ç»Ÿè®¡
    print("\n7. Computing difference statistics...")
    
    differences = trajectory_rpy - trajectory_pytorch
    diff_norms = np.linalg.norm(differences, axis=1)
    
    print(f"\nDifference statistics:")
    print(f"  Mean difference norm: {np.mean(diff_norms):.2e}")
    print(f"  Max difference norm: {np.max(diff_norms):.2e}")
    print(f"  Min difference norm: {np.min(diff_norms):.2e}")
    print(f"  Std difference norm: {np.std(diff_norms):.2e}")
    
    # ç›¸å¯¹è¯¯å·®
    rpy_norms = np.linalg.norm(trajectory_rpy, axis=1)
    relative_errors = diff_norms / (rpy_norms + 1e-10)
    print(f"\nRelative error:")
    print(f"  Mean: {np.mean(relative_errors):.2e}")
    print(f"  Max: {np.max(relative_errors):.2e}")
    
    return trajectory_rpy, trajectory_pytorch, differences

def calculate_effective_dimension(trajectory):
    """è®¡ç®—è½¨è¿¹çš„æœ‰æ•ˆç»´åº¦ï¼ˆParticipation Ratioï¼‰"""
    cov_matrix = np.cov(trajectory.T)
    eigenvalues = np.linalg.eigvalsh(cov_matrix)
    eigenvalues = eigenvalues[eigenvalues > 1e-10]
    
    sum_lambda = np.sum(eigenvalues)
    sum_lambda_squared = np.sum(eigenvalues**2)
    
    if sum_lambda_squared > 0:
        effective_dim = (sum_lambda**2) / sum_lambda_squared
    else:
        effective_dim = 1.0
    
    return effective_dim

def visualize_comparison(trajectory_rpy, trajectory_pytorch, initial_state_type):
    """å¯è§†åŒ–ä¸¤ä¸ªç‰ˆæœ¬çš„è½¨è¿¹å¯¹æ¯”"""
    
    # è®¡ç®—PCA
    pca_rpy = PCA(n_components=3)
    trajectory_rpy_pca = pca_rpy.fit_transform(trajectory_rpy)
    
    pca_pytorch = PCA(n_components=3)
    trajectory_pytorch_pca = pca_pytorch.fit_transform(trajectory_pytorch)
    
    # è®¡ç®—æœ‰æ•ˆç»´åº¦
    eff_dim_rpy = calculate_effective_dimension(trajectory_rpy)
    eff_dim_pytorch = calculate_effective_dimension(trajectory_pytorch)
    
    print(f"\nEffective dimensions:")
    print(f"  ReservoirPy: {eff_dim_rpy:.2f}")
    print(f"  PyTorch: {eff_dim_pytorch:.2f}")
    
    # åˆ›å»ºå›¾å½¢
    fig = plt.figure(figsize=(16, 12))
    
    # 3Dè½¨è¿¹å¯¹æ¯”
    ax1 = fig.add_subplot(2, 3, 1, projection='3d')
    ax1.plot(trajectory_rpy_pca[:, 0], trajectory_rpy_pca[:, 1], trajectory_rpy_pca[:, 2], 
             'b-', alpha=0.6, linewidth=1, label='ReservoirPy')
    ax1.set_title(f'ReservoirPy Trajectory\nEff. Dim: {eff_dim_rpy:.2f}')
    ax1.set_xlabel('PC1')
    ax1.set_ylabel('PC2')
    ax1.set_zlabel('PC3')
    ax1.legend()
    
    ax2 = fig.add_subplot(2, 3, 2, projection='3d')
    ax2.plot(trajectory_pytorch_pca[:, 0], trajectory_pytorch_pca[:, 1], trajectory_pytorch_pca[:, 2], 
             'r-', alpha=0.6, linewidth=1, label='PyTorch')
    ax2.set_title(f'PyTorch Trajectory\nEff. Dim: {eff_dim_pytorch:.2f}')
    ax2.set_xlabel('PC1')
    ax2.set_ylabel('PC2')
    ax2.set_zlabel('PC3')
    ax2.legend()
    
    # å åŠ å¯¹æ¯”
    ax3 = fig.add_subplot(2, 3, 3, projection='3d')
    ax3.plot(trajectory_rpy_pca[:, 0], trajectory_rpy_pca[:, 1], trajectory_rpy_pca[:, 2], 
             'b-', alpha=0.5, linewidth=1, label='ReservoirPy')
    ax3.plot(trajectory_pytorch_pca[:, 0], trajectory_pytorch_pca[:, 1], trajectory_pytorch_pca[:, 2], 
             'r--', alpha=0.5, linewidth=1, label='PyTorch')
    ax3.set_title('Overlaid Trajectories')
    ax3.set_xlabel('PC1')
    ax3.set_ylabel('PC2')
    ax3.set_zlabel('PC3')
    ax3.legend()
    
    # çŠ¶æ€èŒƒæ•°æ¼”åŒ–
    ax4 = fig.add_subplot(2, 3, 4)
    norms_rpy = np.linalg.norm(trajectory_rpy, axis=1)
    norms_pytorch = np.linalg.norm(trajectory_pytorch, axis=1)
    ax4.plot(norms_rpy, 'b-', alpha=0.7, label='ReservoirPy')
    ax4.plot(norms_pytorch, 'r--', alpha=0.7, label='PyTorch')
    ax4.set_xlabel('Time Step')
    ax4.set_ylabel('State Norm')
    ax4.set_title('State Norm Evolution')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # å·®å¼‚æ¼”åŒ–
    ax5 = fig.add_subplot(2, 3, 5)
    differences = trajectory_rpy - trajectory_pytorch
    diff_norms = np.linalg.norm(differences, axis=1)
    ax5.semilogy(diff_norms, 'g-', linewidth=2)
    ax5.set_xlabel('Time Step')
    ax5.set_ylabel('Difference Norm (log scale)')
    ax5.set_title('Numerical Difference Between Implementations')
    ax5.grid(True, alpha=0.3)
    
    # ç›¸å¯¹è¯¯å·®
    ax6 = fig.add_subplot(2, 3, 6)
    relative_errors = diff_norms / (norms_rpy + 1e-10)
    ax6.semilogy(relative_errors, 'orange', linewidth=2)
    ax6.set_xlabel('Time Step')
    ax6.set_ylabel('Relative Error (log scale)')
    ax6.set_title('Relative Error')
    ax6.grid(True, alpha=0.3)
    
    plt.suptitle(f'ReservoirPy vs PyTorch ESN Comparison\n{initial_state_type.capitalize()} Initial State', 
                 fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    return fig

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ESN PyTorch vs ReservoirPy Numerical Comparison")
    print("="*60)
    
    # å®éªŒå‚æ•°ï¼ˆä¸esn_dynamics_visualization.pyç›¸åŒï¼‰
    n_steps = 10000  # å®Œæ•´çš„10000æ­¥å®éªŒ
    input_dim = 96
    reservoir_size = 1000
    spectral_radius = 1.9
    leak_rate = 0.1
    
    # æµ‹è¯•ä¸¤ç§åˆå§‹çŠ¶æ€
    for initial_state_type in ['zero', 'random']:
        print(f"\n{'='*60}")
        print(f"Testing with {initial_state_type} initial state")
        print('='*60)
        
        # è¿è¡Œå¯¹æ¯”å®éªŒ
        trajectory_rpy, trajectory_pytorch, differences = run_comparison_experiment(
            n_steps=n_steps,
            input_dim=input_dim,
            reservoir_size=reservoir_size,
            spectral_radius=spectral_radius,
            leak_rate=leak_rate,
            initial_state_type=initial_state_type
        )
        
        # å¯è§†åŒ–å¯¹æ¯”
        fig = visualize_comparison(trajectory_rpy, trajectory_pytorch, initial_state_type)
        plt.savefig(f'esn_pytorch_comparison_{initial_state_type}.png', dpi=150, bbox_inches='tight')
        
        # éªŒè¯æ•°å€¼ä¸€è‡´æ€§
        max_diff = np.max(np.abs(differences))
        mean_diff = np.mean(np.abs(differences))
        
        print(f"\n{'='*60}")
        print(f"Numerical Consistency Check ({initial_state_type} initial state):")
        print(f"  Maximum absolute difference: {max_diff:.2e}")
        print(f"  Mean absolute difference: {mean_diff:.2e}")
        
        if max_diff < 1e-10:
            print("  âœ“ EXCELLENT: Implementations are numerically identical (diff < 1e-10)")
        elif max_diff < 1e-6:
            print("  âœ“ GOOD: Implementations are very close (diff < 1e-6)")
        elif max_diff < 1e-3:
            print("  âš  WARNING: Small differences detected (diff < 1e-3)")
        else:
            print("  âœ— ERROR: Significant differences detected!")
    
    plt.show()
    
    print("\n" + "="*60)
    print("Comparison completed!")
    print("Results saved to: esn_pytorch_comparison_*.png")
    print("="*60)

if __name__ == "__main__":
    main()
